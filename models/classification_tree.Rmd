---
title: "Classification Tree"
author: "Khalyl Smith & David Oloyede"
date: "4/11/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
stroke.data <- read.csv("../stroke-data.csv", na.strings="N/A", stringsAsFactors = TRUE)
stroke.data = na.omit(stroke.data); stroke.data = stroke.data[-1]
stroke.data$stroke = as.factor(stroke.data$stroke)
```

# Classification Tree Model

For the classification tree, we'll use `stroke` as our response variable and all other variables (excluding for `id`) as our predictors. This is the formula we will use for the classification tree:
\begin{align*}
& \hat{stroke} \sim gender + age + hypertension + heart\_disease + ever\_married + \\
& work\_type + Residence\_type + avg\_glucose\_level + bmi + smoking\_status 
\end{align*}

Just like we did for the logistic regression model, we need to undersample our data to retrieve desirable results.

```{r, message = FALSE, warning=FALSE}
library(caret)
set.seed(5)
stroke.data2 = downSample(stroke.data[,-c(11)], stroke.data$stroke, list = FALSE, yname = "stroke")
```

Now we can split our data into a training set and testing set with an 80-20 split to get this tree:

```{r}
train = sample(nrow(stroke.data2), round(nrow(stroke.data2)*.80))
test.stroke = stroke.data2[-train,]
tree.stroke = tree::tree(stroke~., data = stroke.data2, subset = train)
```

```{r, fig.align = 'center', out.height = "35%", echo = FALSE}
plot(tree.stroke); text(tree.stroke, pretty = 1)
summary(tree.stroke)
```

For our undersampled tree, it looks like `age`, `bmi`, `hypertension`, `avg_glucose_level`, and `smoking_status` are present in this tree. We have a residual mean deviance of `0.7606` and a misclassification error rate of `17.96%`. With 15 terminal nodes, we should prune the tree using cross-validation which we will do in the cross-validation section of this report. Although in the case of cross-validation, we will need to account for the new undersampled data. But for now, let's evaluate the performance of our tree by using the testing set of our data and calculating the accuracy rate.

```{r}
tree.pred = predict(tree.stroke, test.stroke, type = 'class')
table(tree.pred, test.stroke$stroke)
```

The accuracy rate is $0.7738095 \approx$ `77.4%` and conversely a test error rate of $0.2261905 \approx$ `22.62%`. With this accuracy rate, our tree performs fairly well in predicting if a patient may be in risk of getting a stroke. Let's repeat this process ten times with different subsets of training and testing data and calculate the mean of each test prediction errors.

```{r, echo = FALSE}
test.errors = NA

for (i in 1:10) {
  train = sample(nrow(stroke.data2), round(nrow(stroke.data2)*.80))
  test.stroke = stroke.data2[-train,]
  tree.stroke = tree::tree(stroke~., data = stroke.data2, subset = train)
  tree.pred = predict(tree.stroke, test.stroke, type = 'class')

  test.matrix = table(tree.pred, test.stroke$stroke)
  
  test.errors[i] = (test.matrix[2] + test.matrix[3])/sum(test.matrix)
}

```

After iterating ten times, we receive the following test error rates and their corresponding mean:

```{r}
test.errors
mean(test.errors)
```

This mean of our test error rates is slightly higher than the one we received when we performed our first sampling (`22.62%`) which supports the claim that our data and model performs well in predicting if a patient is in risk of receiving a stroke. However, we still need to consider pruning for better results.

Compared to the logistic regression model's mean of test error rates, `25%`, our tree model performs slightly better than the logistic regression approach.